{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from ml_metrics import rmsle\n",
    "import xgboost as xgb\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import datasets, linear_model\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_features(bimbo_dataframe):\n",
    "  \"\"\"This function takes an input dataframe and returns a version of it that has\n",
    "  various features selected and pre-processed.  The input dataframe contains\n",
    "  data from the california_housing data set.\"\"\"\n",
    "  # Select fewer columns to allow training a bit faster.\n",
    "  output_features = bimbo_dataframe[\n",
    "    [\"d1\",\n",
    "     \"d2\",\n",
    "     \"d3\",\n",
    "     \"d4\",\n",
    "     \"d5\",\n",
    "     \"d6\",\n",
    "     \"p_total_demand\",\n",
    "     \"total_demand\"]].copy()\n",
    "  return output_features\n",
    "\n",
    "\n",
    "def preprocess_targets(bimbo_dataframe):\n",
    "  \"\"\"This function selects and potentially transforms the output target from\n",
    "  an input dataframe containing data from the california_housing data set.\n",
    "  The object returned is a pandas Series.\"\"\"\n",
    "  output_targets = pd.DataFrame()\n",
    "  # Scale the target to be in units of thousands of dollars.\n",
    "  output_targets[\"Demanda_uni_equil\"] = (\n",
    "    bimbo_dataframe[\"Demanda_uni_equil\"])\n",
    "  return output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_joined = pd.read_csv('../input/train_1000000_alld_totals.csv')\n",
    "test_joined = pd.read_csv('../input/test_1000000_alld_totals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomize the data before selecting train / validation splits.\n",
    "raw_training_df = train_joined.reindex(np.random.permutation(\n",
    "  train_joined.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_examples = preprocess_features(raw_training_df.head(100000))\n",
    "training_targets = preprocess_targets(raw_training_df.head(100000))\n",
    "\n",
    "validation_examples = preprocess_features(raw_training_df.tail(500))\n",
    "validation_targets = preprocess_targets(raw_training_df.tail(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_examples=training_examples.replace(-999,np.NaN)\n",
    "validation_examples=training_examples.replace(-999,np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples summary:\n",
      "                 d1            d2            d3            d4            d5  \\\n",
      "count  85042.000000  70237.000000  55974.000000  42293.000000  28231.000000   \n",
      "mean       5.614896      4.789541      4.686765      4.588726      4.461514   \n",
      "std       26.031033     17.158641     18.098319     16.162761     16.162597   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        2.000000      1.000000      0.000000      0.000000      0.000000   \n",
      "75%        5.000000      4.000000      4.000000      4.000000      4.000000   \n",
      "max     3960.000000   1089.000000   1695.000000    806.000000    710.000000   \n",
      "\n",
      "                 d6  p_total_demand   total_demand  \n",
      "count  14161.000000    1.000000e+05  100000.000000  \n",
      "mean       4.427159    4.915619e+06    1547.898540  \n",
      "std       17.374971    5.352383e+06    5129.317755  \n",
      "min        0.000000    0.000000e+00       0.000000  \n",
      "25%        0.000000    8.954640e+05     350.000000  \n",
      "50%        0.000000    3.011337e+06     774.000000  \n",
      "75%        4.000000    7.229285e+06    1580.000000  \n",
      "max      800.000000    2.372867e+07  893756.000000  \n",
      "Validation examples summary:\n",
      "                 d1            d2            d3            d4            d5  \\\n",
      "count  85042.000000  70237.000000  55974.000000  42293.000000  28231.000000   \n",
      "mean       5.614896      4.789541      4.686765      4.588726      4.461514   \n",
      "std       26.031033     17.158641     18.098319     16.162761     16.162597   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        2.000000      1.000000      0.000000      0.000000      0.000000   \n",
      "75%        5.000000      4.000000      4.000000      4.000000      4.000000   \n",
      "max     3960.000000   1089.000000   1695.000000    806.000000    710.000000   \n",
      "\n",
      "                 d6  p_total_demand   total_demand  \n",
      "count  14161.000000    1.000000e+05  100000.000000  \n",
      "mean       4.427159    4.915619e+06    1547.898540  \n",
      "std       17.374971    5.352383e+06    5129.317755  \n",
      "min        0.000000    0.000000e+00       0.000000  \n",
      "25%        0.000000    8.954640e+05     350.000000  \n",
      "50%        0.000000    3.011337e+06     774.000000  \n",
      "75%        4.000000    7.229285e+06    1580.000000  \n",
      "max      800.000000    2.372867e+07  893756.000000  \n",
      "Training targets summary:\n",
      "       Demanda_uni_equil\n",
      "count      100000.000000\n",
      "mean            7.013910\n",
      "std            19.860189\n",
      "min             0.000000\n",
      "25%             2.000000\n",
      "50%             3.000000\n",
      "75%             6.000000\n",
      "max          2640.000000\n",
      "Validation targets summary:\n",
      "       Demanda_uni_equil\n",
      "count         500.000000\n",
      "mean            6.496000\n",
      "std            13.129156\n",
      "min             0.000000\n",
      "25%             2.000000\n",
      "50%             3.000000\n",
      "75%             7.000000\n",
      "max           164.000000\n"
     ]
    }
   ],
   "source": [
    "# Sanity check that we've done the right thing.\n",
    "print(\"Training examples summary:\")\n",
    "print(training_examples.describe())\n",
    "print(\"Validation examples summary:\")\n",
    "print(validation_examples.describe())\n",
    "\n",
    "print(\"Training targets summary:\")\n",
    "print(training_targets.describe())\n",
    "print(\"Validation targets summary:\")\n",
    "print(validation_targets.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1 = tf.contrib.layers.real_valued_column(\"d1\")\n",
    "d2 = tf.contrib.layers.real_valued_column(\"d2\")\n",
    "d3 = tf.contrib.layers.real_valued_column(\"d3\")\n",
    "d4 = tf.contrib.layers.real_valued_column(\"d4\")\n",
    "d5 = tf.contrib.layers.real_valued_column(\"d5\")\n",
    "d6 = tf.contrib.layers.real_valued_column(\"d6\")\n",
    "\n",
    "p_total_demand = tf.contrib.layers.real_valued_column(\"p_total_demand\")\n",
    "total_demand = tf.contrib.layers.real_valued_column(\"total_demand\")\n",
    "\n",
    "\n",
    "feature_columns=set([\n",
    "  d1,\n",
    "  d2,\n",
    "  d3,\n",
    "  d4,\n",
    "  d5,\n",
    "  d6,\n",
    "  p_total_demand,\n",
    "  total_demand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "def _input_fn(examples_df, targets_df):\n",
    "  # Converts a pair of examples/targets DataFrames to Tensors. The Tensors are\n",
    "  # reshaped into (N,1) where N is number of examples in the DataFrames.\n",
    "  # -1 is a special value to tf.reshape that means \"replace me with whatever\n",
    "  # size allows all the input values to fit into the output tensor.\"\n",
    "  features = {}\n",
    "  for column_name in examples_df.keys():\n",
    "    features[column_name] = tf.to_float(\n",
    "      tf.reshape(tf.constant(examples_df[column_name].values), [-1,1]))\n",
    "  label_tensor = tf.to_float(\n",
    "    tf.reshape(tf.constant(targets_df[targets_df.keys()[0]].values), [-1,1]))\n",
    "\n",
    "  # Return a dict of feature Tensors and label Tensor.\n",
    "  return features, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "RMSE:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'d1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-11377a1d4632>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtraining_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m   )\n\u001b[0;32m     28\u001b[0m   \u001b[0mpredictions_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnn_regressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, input_fn, steps, batch_size, monitors)\u001b[0m\n\u001b[0;32m    180\u001b[0m                              \u001b[0mfeed_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                              \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                              monitors=monitors)\n\u001b[0m\u001b[0;32m    183\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss)\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m       \u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m       \u001b[1;31m# Add default monitors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.pyc\u001b[0m in \u001b[0;36m_get_train_ops\u001b[1;34m(self, features, targets)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dnn_feature_columns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dnn_feature_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_real_valued_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDNNRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.pyc\u001b[0m in \u001b[0;36m_get_train_ops\u001b[1;34m(self, features, targets)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0mglobal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrib_variables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m     with ops.control_dependencies([self._centered_bias_step(\n\u001b[0;32m    158\u001b[0m         targets, self._get_weight_tensor(features))]):\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.pyc\u001b[0m in \u001b[0;36m_logits\u001b[1;34m(self, features, is_training)\u001b[0m\n\u001b[0;32m    296\u001b[0m                 self._dnn_logits(features, is_training=is_training))\n\u001b[0;32m    297\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_dnn_feature_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m       \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dnn_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m       \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.pyc\u001b[0m in \u001b[0;36m_dnn_logits\u001b[1;34m(self, features, is_training)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_dnn_feature_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         weight_collections=[self._dnn_weight_collection])\n\u001b[0m\u001b[0;32m    223\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hidden_units\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dnn_hidden_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m       net = layers.legacy_fully_connected(\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.pyc\u001b[0m in \u001b[0;36minput_from_feature_columns\u001b[1;34m(columns_to_tensors, feature_columns, weight_collections, name, trainable)\u001b[0m\n\u001b[0;32m     98\u001b[0m                                     [ops.GraphKeys.VARIABLES]))\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m       \u001b[0mtransformed_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m       output_tensors.append(column.to_dnn_input_layer(\n\u001b[0;32m    102\u001b[0m           transformed_tensor, weight_collections, trainable))\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, feature_column)\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns_to_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert_transformed_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns_to_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfeature_column\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns_to_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/can/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/feature_column.pyc\u001b[0m in \u001b[0;36minsert_transformed_feature\u001b[1;34m(self, columns_to_tensors)\u001b[0m\n\u001b[0;32m    570\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minsert_transformed_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns_to_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;31m# No transformation is needed for _RealValuedColumn except reshaping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns_to_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'd1'"
     ]
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\", \"timeout\": 180}\n",
    "\n",
    "LEARNING_RATE = 0.0007  # @param\n",
    "STEPS = 5000  # @param\n",
    "BATCH_SIZE = 70  # @param\n",
    "HIDDEN_UNITS = [10, 10]  # @param\n",
    "periods = 10\n",
    "steps_per_period = STEPS / periods\n",
    "\n",
    "# Set up our NN with the desired learning settings.\n",
    "dnn_regressor = tf.contrib.learn.DNNRegressor(\n",
    "  feature_columns=feature_columns,\n",
    "  hidden_units=HIDDEN_UNITS,\n",
    "  optimizer=tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE),\n",
    "#  gradient_clip_norm=5.0\n",
    ")\n",
    "print \"Training model...\"\n",
    "print \"RMSE:\"\n",
    "root_mean_squared_errors_training = []\n",
    "root_mean_squared_errors_validation = []\n",
    "for period in range (0, periods):\n",
    "  dnn_regressor.fit(\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    steps=steps_per_period,\n",
    "    batch_size=BATCH_SIZE\n",
    "  )\n",
    "  predictions_validation = dnn_regressor.predict(validation_examples)\n",
    "  predictions_training = dnn_regressor.predict(training_examples)\n",
    "\n",
    "  root_mean_squared_error_validation = math.sqrt(metrics.mean_squared_error(\n",
    "    predictions_validation, validation_targets))\n",
    "  root_mean_squared_error_training = math.sqrt(metrics.mean_squared_error(\n",
    "    predictions_training, training_targets))\n",
    "\n",
    "  root_mean_squared_errors_validation.append(root_mean_squared_error_validation)\n",
    "  root_mean_squared_errors_training.append(root_mean_squared_error_training)\n",
    "\n",
    "  print \"  period %02d : %3.2f\" % (period, root_mean_squared_error_training)\n",
    "\n",
    "# Output a graph of loss metrics over periods.\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Periods\")\n",
    "plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "plt.plot(root_mean_squared_errors_training, label='training')\n",
    "plt.plot(root_mean_squared_errors_validation, label='validation')\n",
    "plt.legend()\n",
    "\n",
    "# Display some summary information.\n",
    "print \"Final RMSE (on training data):   %0.2f\" % root_mean_squared_error_training\n",
    "print \"Final RMSE (on validation data): %0.2f\" % root_mean_squared_error_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Periods\")\n",
    "plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "plt.tight_layout()\n",
    "plt.plot(training_rmse, label=\"training\")\n",
    "plt.plot(validation_rmse, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Final RMSE (on training data): %0.2f\" % (\n",
    "  training_root_mean_squared_error))\n",
    "print(\"Final RMSE (on validation data): %0.2f\" % (\n",
    "  validation_root_mean_squared_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
