{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '../input'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1733e8c69ce3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'MB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 2] No such file or directory: '../input'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import display_markdown as mkdown # as print\n",
    "\n",
    "def nl():\n",
    "    print('\\n')\n",
    "\n",
    "for f in os.listdir('../input'):\n",
    "    print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It looks like we are given quite a few sets as an input! Let's take a look at each one, starting with train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Size of training set: (500000, 11)\n",
      " Size of testing set: (500000, 7)\n",
      "\n",
      "\n",
      "Columns in train: ['Semana', 'Agencia_ID', 'Canal_ID', 'Ruta_SAK', 'Cliente_ID', 'Producto_ID', 'Venta_uni_hoy', 'Venta_hoy', 'Dev_uni_proxima', 'Dev_proxima', 'Demanda_uni_equil']\n",
      " Columns in test: ['id', 'Semana', 'Agencia_ID', 'Canal_ID', 'Ruta_SAK', 'Cliente_ID', 'Producto_ID']\n",
      "\n",
      "\n",
      "         Semana     Agencia_ID       Canal_ID       Ruta_SAK    Cliente_ID  \\\n",
      "count  500000.0  500000.000000  500000.000000  500000.000000  5.000000e+05   \n",
      "mean        3.0    1115.814010       1.118138    1590.864712  1.198001e+06   \n",
      "std         0.0       3.127955       0.823334     801.433471  1.462295e+06   \n",
      "min         3.0    1110.000000       1.000000       1.000000  3.967000e+03   \n",
      "25%         3.0    1112.000000       1.000000    1109.000000  1.098650e+05   \n",
      "50%         3.0    1117.000000       1.000000    1447.000000  5.137590e+05   \n",
      "75%         3.0    1118.000000       1.000000    1606.000000  1.793000e+06   \n",
      "max         3.0    1120.000000      11.000000    4501.000000  9.747854e+06   \n",
      "\n",
      "         Producto_ID  Venta_uni_hoy      Venta_hoy  Dev_uni_proxima  \\\n",
      "count  500000.000000  500000.000000  500000.000000    500000.000000   \n",
      "mean    13717.625918       6.961222      73.172699         0.113388   \n",
      "std     17232.601091      17.450644     276.414112         2.367468   \n",
      "min        72.000000       0.000000       0.000000         0.000000   \n",
      "25%      1216.000000       2.000000      19.660000         0.000000   \n",
      "50%      1284.000000       4.000000      36.320000         0.000000   \n",
      "75%     32322.000000       7.000000      67.040000         0.000000   \n",
      "max     49994.000000    2000.000000   21736.560000      1008.000000   \n",
      "\n",
      "         Dev_proxima  Demanda_uni_equil  \n",
      "count  500000.000000      500000.000000  \n",
      "mean        1.452218           6.877256  \n",
      "std        26.438610          17.238430  \n",
      "min         0.000000           0.000000  \n",
      "25%         0.000000           2.000000  \n",
      "50%         0.000000           4.000000  \n",
      "75%         0.000000           7.000000  \n",
      "max      9426.000000        2000.000000  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../input/train.csv', nrows=500000)\n",
    "df_test = pd.read_csv('../input/test.csv', nrows=500000)\n",
    "\n",
    "nl()\n",
    "print('Size of training set: ' + str(df_train.shape))\n",
    "print(' Size of testing set: ' + str(df_test.shape))\n",
    "\n",
    "nl()\n",
    "print('Columns in train: ' + str(df_train.columns.tolist()))\n",
    "print(' Columns in test: ' + str(df_test.columns.tolist()))\n",
    "\n",
    "nl()\n",
    "print(df_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Demanda_uni_equil` is the target value that we are trying to predict.\n",
    "\n",
    "Let's take a look at the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = df_train['Demanda_uni_equil'].tolist()\n",
    "\n",
    "def label_plot(title, x, y):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "\n",
    "plt.hist(target, bins=200, color='blue')\n",
    "label_plot('Distribution of target values', 'Demanda_uni_equil', 'Count')\n",
    "plt.show()\n",
    "\n",
    "print(\"Looks like we have some pretty big outliers, let's zoom in and try again\")\n",
    "\n",
    "print('Data with target values under 50: ' + str(round(len(df_train.loc[df_train['Demanda_uni_equil'] <= 50]) / 5000, 2)) + '%')\n",
    "\n",
    "plt.hist(target, bins=50, color='blue', range=(0, 50))\n",
    "label_plot('Distribution of target values under 50', 'Demanda_uni_equil', 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this distribution, we can see that some target values are much more common than others.\n",
    "\n",
    "Let's find the mode of the target and make a naive submission using that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(target).most_common(10))\n",
    "print('Our most common value is 2')\n",
    "\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub['Demanda_uni_equil'] = 2\n",
    "sub.to_csv('mostcommon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, our script (0.96080) performs worse than submitting `6` as the predicted value. This could be for two reasons:\n",
    "\n",
    "1) Our values are incorrect since we have only read the first 500,000 values of the dataset and the set is not randomised.  \n",
    "2) Due to the [evaluation metric](https://www.kaggle.com/c/grupo-bimbo-inventory-demand/details/evaluation) predicting 6 actually gives a lower overall logarithmic error.\n",
    "\n",
    "We will begin by investigating the first possibility, and will look at whether the time-series has any effect on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pseudo_time = df_train.loc[df_train.Demanda_uni_equil < 20].index.tolist()\n",
    "target = df_train.loc[df_train.Demanda_uni_equil < 20].Demanda_uni_equil.tolist()\n",
    "\n",
    "plt.hist2d(pseudo_time, target, bins=[50,20])\n",
    "label_plot('Histogram of target value over index', 'Index', 'Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not look like the time-series has much effect on the data, except for that anomaly around 200k (we may take a closer look at another time)\n",
    "\n",
    "To test out option 2, I created a script which evaluates the RMSLE on the training set to try and find the best value to submit, and it scored 0.82735:  \n",
    "https://www.kaggle.com/anokas/grupo-bimbo-inventory-demand/optimised-beat-the-benchmark\n",
    "\n",
    "Now that we have found the best naive submission to make, we can go onto looking at the other columns!\n",
    "\n",
    "We will begin by looking at the time column, semana (meaning week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "semana = df_train['Semana']\n",
    "print(semana.value_counts())\n",
    "print('\\nIt looks like by sampling only the first 500,000 columns, we have only sampled from week 3.\\nWe will have to take a larger portion of the dataset\\n')\n",
    "\n",
    "timing = pd.read_csv('../input/train.csv', usecols=['Semana','Demanda_uni_equil'])\n",
    "print('Size: ' + str(timing.shape))\n",
    "\n",
    "print(timing['Semana'].value_counts())\n",
    "plt.hist(timing['Semana'].tolist(), bins=7, color='red')\n",
    "label_plot('Distribution of weeks in training data', 'Semana', 'Frequency')\n",
    "plt.show()\n",
    "\n",
    "timing_test = pd.read_csv('../input/test.csv', usecols=['Semana'])\n",
    "print(timing_test['Semana'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a different set of weeks in the testing data for us to predict - meaning that this is likely a time series prediction problem for each of the product/client/location pairs in train and test sets.\n",
    "\n",
    "Since this appears to be a time series prediction task, let's see if there are any trends in the target value over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timing = timing.sample(1000000)\n",
    "timing = timing.loc[timing['Demanda_uni_equil'] < 15] # We only want to look at the most common values\n",
    "\n",
    "plt.hist2d(timing['Semana'].tolist(), timing['Demanda_uni_equil'].tolist(), bins=[7, 15])\n",
    "label_plot('Distribution of target value over time', 'Week', 'Target')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}